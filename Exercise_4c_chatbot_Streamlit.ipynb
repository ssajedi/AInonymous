{"cells":[{"cell_type":"markdown","metadata":{"id":"z_ImLlAI6qZB"},"source":["# Building a Chatbot\n","## Part III: Building a chatbot UI\n","\n","### CORE Studio, Thornton Tomasetti\n","\n","#### Instructor: [Seyedomid Sajedi](https://www.linkedin.com/in/seyedomid-sajedi-263b703a)\n","Part I and II of exercise 4 taught us the mechanics of instructing the chatbot to answer user queries based on a custom databse that we define.\n","\n","An important part of any machine learning project is how you share it with other people. We spent all our time executing code snippets in this workshop. For our final notebook, we will look at how [Streamlit](https://streamlit.io/) can help us build a good looking user interface for our chatbot. As expected, we need to bring the modules that we created in parts I and II and organize them.\n"]},{"cell_type":"markdown","metadata":{"id":"rF3q72rbIMjG"},"source":["##Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":13309,"status":"ok","timestamp":1697658244653,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"UoPsaglfJqNA"},"outputs":[],"source":["%%capture\n","!pip install rank_bm25 pypdf2 tiktoken openai"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697658277129,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"pjwUaCALJx-N"},"outputs":[],"source":["# https://spacy.io/\n","import spacy\n","# !pip install --upgrade spacy # might be needed if the default spacy in colab is not working\n","import requests\n","from io import BytesIO\n","from PyPDF2 import PdfReader\n","from tqdm import tqdm\n","import tiktoken\n","from rank_bm25 import BM25Okapi\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pprint\n","import pickle"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1697658280191,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"no0keP8BXK8l"},"outputs":[],"source":["nlp_model = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1697658280191,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"0U2t5SldW7MF"},"outputs":[],"source":["def spacy_tokenizer(text,nlp):\n","    doc = nlp(text)\n","    tokens = []\n","    for token in doc:\n","        # Check if the token is not punctuation and not a stop word, or if it's a stop word in all caps\n","        if (not token.is_punct) :\n","            # check if token letters are all cap\n","            all_cap_cond = all(c.isupper() for c in token.text)\n","\n","            if not token.is_stop or ((token.is_stop) and (token.pos_ == 'PROPN' or all_cap_cond)):\n","                tokens.append(token.lemma_.lower())\n","    return tokens\n","# We used this function for topic modeling\n","def get_pdf_as_memory_stream(url):\n","    response = requests.get(url)\n","    response.raise_for_status()  # Raise an error for HTTP errors\n","\n","    # Convert the response content into a BytesIO stream\n","    return BytesIO(response.content)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31301,"status":"ok","timestamp":1697658313020,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"1LAbwtFoU_xC","outputId":"113c2261-1c51-4516-eedf-7c5465b72015"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of pages: 184\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 184/184 [00:30<00:00,  6.04it/s]\n"]}],"source":["# Ref: https://www.nyc.gov/site/buildings/codes/2022-construction-codes.page#bldgs\n","# make sure to click on the Local Law 77 from each chapter to get the pdf file.\n","pdf_url = r\"https://www.nyc.gov/assets/buildings/local_laws/ll77of2023.pdf\"\n","pdf_file = get_pdf_as_memory_stream(pdf_url)\n","reader = PdfReader(pdf_file)\n","# number of pages\n","n_pages = len(reader.pages)\n","print(f\"Number of pages: {n_pages}\")\n","# tokenize dataset\n","tokenized_dataset= []\n","text_dataset =[]\n","for page in tqdm(reader.pages,total=n_pages):\n","  text = page.extract_text()\n","  text_dataset.append(text)\n","  tokenized_page = spacy_tokenizer(text,nlp_model)\n","  tokenized_dataset.append(tokenized_page)\n","# save the dataset to a file to make it accecible for the chatbot\n","with open('dataset.pkl', 'wb') as f:\n","    pickle.dump({'text_dataset':text_dataset,\n","                 'tokenized_pages':tokenized_dataset},\n","                f)"]},{"cell_type":"markdown","metadata":{"id":"8m-MW-BSIY9I"},"source":["## Streamlit\n","Streamlit requires all your python script inside a py file. The first line is a magic command that will write all the instructions we have had before into a single `chatbot.py` file. Let's quickly review the code together."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1697658668250,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"NL6iOjFCXdXi","outputId":"686a3296-a7ee-4e71-dfc4-6fb362bfa526"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting chatbot.py\n"]}],"source":["%%writefile chatbot.py\n","# load libraries\n","import streamlit as st\n","import spacy\n","from rank_bm25 import BM25Okapi\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import streamlit as st\n","import pickle\n","import openai\n","import os\n","\n","# \"\"\"\n","#   ____  _               _\n","#  / ___|| |_ ___ _ __   / |\n","#  \\___ \\| __/ _ \\ '_ \\  | |\n","#   ___) | ||  __/ |_) | | |\n","#  |____/ \\__\\___| .__/  |_|\n","#                |_|\n","# We define functions that load processed data and models into cache using streamlit's caching mechanism.\n","# The cache command make the streamlit app faster and more responsive.\n","# \"\"\"\n","@st.cache_resource\n","def init_bm25_vectorizer():\n","    \"\"\"Loads the bm25 vectorizer and puts it into cache\"\"\"\n","    global data_dict\n","    return BM25Okapi(data_dict['tokenized_pages'])\n","\n","@st.cache_resource\n","def load_spacy_model():\n","    \"\"\"Loads spacy tokenizer and puts it into cache\"\"\"\n","    return spacy.load(\"en_core_web_sm\")\n","\n","@st.cache_data\n","def load_data():\n","    \"\"\"Loads the saved and processed data from the pickle file and stores in into cache\"\"\"\n","    with open('dataset.pkl', 'rb') as f:\n","        data_dict = pickle.load(f)\n","    return data_dict\n","# \"\"\"\n","\n","#   ____  _               ____\n","#  / ___|| |_ ___ _ __   |___ \\\n","#  \\___ \\| __/ _ \\ '_ \\    __) |\n","#   ___) | ||  __/ |_) |  / __/\n","#  |____/ \\__\\___| .__/  |_____|\n","#                |_|\n","# We are organizing the functions that we defined in earlier steps of example 4.\n","# \"\"\"\n","# Utility functions\n","def spacy_tokenizer(text,nlp):\n","    doc = nlp(text)\n","    tokens = []\n","    for token in doc:\n","        # Check if the token is not punctuation and not a stop word, or if it's a stop word in all caps\n","        if (not token.is_punct) :\n","            # check if token letters are all cap\n","            all_cap_cond = all(c.isupper() for c in token.text)\n","\n","            if not token.is_stop or ((token.is_stop) and (token.pos_ == 'PROPN' or all_cap_cond)):\n","                tokens.append(token.lemma_.lower())\n","    return tokens\n","\n","def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n","    \"\"\"This function handles calls to the openai api and returns the response from the chatgpt model\"\"\"\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature, # this is the degree of randomness of the model's output\n","    )\n","    return response.choices[0].message[\"content\"]\n","\n","def doc_retrieval(query,pages,\n","                  top_k=3):\n","  #  we need to load bm25_vectorizer,nlp_model in other scripts\n","  query_tokens = spacy_tokenizer(query,nlp_model)\n","  bm25_scores = bm25_vectorizer.get_scores(query_tokens)\n","  top_page_indx= np.argsort(bm25_scores)[-top_k:][::-1]\n","  hits = [{'page_indx': idx, 'score': bm25_scores[idx]} for idx in top_page_indx]\n","  ref_docs = [pages[hit['page_indx']] for hit in hits]\n","  return hits,ref_docs\n","\n","def augment_prompt(prompt):\n","    doc_indx, ref_docs = doc_retrieval(prompt,data_dict['text_dataset'])\n","    aug_prompt = f\"***{prompt}***\"\n","    for doc in ref_docs:\n","        aug_prompt+=f\"```{doc}```\"\n","    st.session_state.chat_hist.append({'role':'user', 'content':prompt,'ref_docs':ref_docs})\n","    return aug_prompt\n","\n","# \"\"\"\n","#   ____  _               _____\n","#  / ___|| |_ ___ _ __   |___ /\n","#  \\___ \\| __/ _ \\ '_ \\    |_ \\\n","#   ___) | ||  __/ |_) |  ___) |\n","#  |____/ \\__\\___| .__/  |____/\n","#                |_|\n","\n","# Loading and caching data and models, adding openai credentials and defining the system prompt.\n","# \"\"\"\n","# Initialize the data and models\n","data_dict = load_data()\n","nlp_model = load_spacy_model()\n","bm25_vectorizer = init_bm25_vectorizer()\n","# get openai api key\n","with open(\"secret_workshop.txt\", \"r\") as f: # this is not the most secure way of adding secret key. For large scale deployment talk to your IT\n","    secret = f.read()\n","openai.api_key = secret\n","\n","\n","\n","system_prompt=\"\"\"You are a helpful assistant named Gary, your task is to review a series of\\\n","documents returned by a search system and answer the user's question only based on these documents.\\\n","The first user query is delimited by triple asterisks\\.\n","The reference documents in that message are delimited with triple backticks.\\\n","A user might ask follow up questions.\n","\"\"\"\n","\n","# \"\"\"\n","#   ____  _               _  _\n","#  / ___|| |_ ___ _ __   | || |\n","#  \\___ \\| __/ _ \\ '_ \\  | || |_\n","#   ___) | ||  __/ |_) | |__   _|\n","#  |____/ \\__\\___| .__/     |_|\n","#                |_|\n","\n","# Streamlit chat interface.\n","# \"\"\"\n","\n","# Building a front end with streamlit\n","# ref: https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps\n","st.title(\"AEC Workshop chatbot\")\n","\n","if \"openai_model\" not in st.session_state:\n","    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","    st.session_state.chat_hist = []\n","\n","for message in st.session_state.chat_hist:\n","    with st.chat_message(message[\"role\"]):\n","        st.markdown(message[\"content\"])\n","\n","if prompt := st.chat_input(\"What is up?\"):\n","    if len(st.session_state.chat_hist)==0:\n","        llm_prompt = augment_prompt(prompt)\n","    else:\n","        llm_prompt = prompt\n","\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": llm_prompt})\n","    with st.chat_message(\"user\"):\n","        st.markdown(prompt)\n","\n","    with st.chat_message(\"assistant\"):\n","        message_placeholder = st.empty()\n","        full_response = \"\"\n","        for response in openai.ChatCompletion.create(\n","            model=st.session_state[\"openai_model\"],\n","            messages=[\n","                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n","                for m in st.session_state.messages\n","            ],\n","            stream=True,\n","        ):\n","            full_response += response.choices[0].delta.get(\"content\", \"\")\n","            message_placeholder.markdown(full_response + \"▌\")\n","        message_placeholder.markdown(full_response)\n","    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n","    st.session_state.chat_hist.append({'role':'assistant', 'content':full_response})\n","\n","# print references:\n","# add a collapsible section to show reference documents\n","if len(st.session_state.chat_hist)>0:\n","    with st.expander(\"References\"):\n","        st.markdown(\"Reference documents:\")\n","        for i,doc in enumerate(st.session_state.chat_hist[0]['ref_docs']):\n","            st.write(f\"Reference {i+1}\")\n","            st.write(\"-\"*20)\n","            st.write(doc)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WLZZW4YDI8nG"},"source":["##Building a local server\n","The following lines are required to run a streamlit app in google colab. If you run this notebook localy, you only need to open an Anaconda propmt and type:\n","`streamlit run chatbot.py` while changing your directory (`cd`) to the path of chatbot.py."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13656,"status":"ok","timestamp":1697658701377,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"k89oS1zaXh0D","outputId":"f66fdad5-8ad2-436e-ec2d-b9fb7721c8e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","added 22 packages from 22 contributors and audited 22 packages in 3.027s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found \u001b[92m0\u001b[0m vulnerabilities\n","\n","\u001b[K\u001b[?25h"]}],"source":["!pip install -q streamlit\n","!npm install localtunnel"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":307,"status":"ok","timestamp":1697658701682,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"HDINpCDn3xo_"},"outputs":[],"source":["!streamlit run chatbot.py &>/content/logs.txt &"]},{"cell_type":"markdown","metadata":{"id":"l8aKoVcJJmSS"},"source":["This step is also needed because we are running the app on google colab. Please copy the endpoint ip and use it after clicking on the generated url."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697658701682,"user":{"displayName":"Omid Sajedi","userId":"17521923881528445598"},"user_tz":240},"id":"gUzO0pu64Jlb","outputId":"bf563f10-5d49-44b8-dfce-12892695c08d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Password/Enpoint IP for localtunnel is: 34.141.245.79\n"]}],"source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"]},{"cell_type":"markdown","metadata":{"id":"v-obXLxl6XXa"},"source":["Make sure secret_workshop.txt is uploaded to the files section of colab before running the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqylp_kk4N8a","outputId":"c369e781-cbe5-419f-b278-cfd8ad4c59b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25hnpx: installed 22 in 2.512s\n","your url is: https://petite-emus-nail.loca.lt\n"]}],"source":["!npx localtunnel --port 8501"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VCS_sQGcV9gm"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMaAFcpZ+hosLK65nqQEdGW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
